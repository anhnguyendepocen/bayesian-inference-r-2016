<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Bayesian Inference in R</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Introduction to Bayesian Inference in R
## Robert Dodier

This document and associated material is on Github:
[robert-dodier/bayesian-inference-r-2016](https://github.com/robert-dodier/bayesian-inference-r-2016)

---

# What is Bayesian inference?

 * Bayesian inference = laws of probability applied to any proposition

 * Proposition = statement about uncertain variables

 * Variables = observable quantities, model parameters, latent (hidden) variables, hypotheses, etc

 * Fundamental operation is to compute conditional probabilities

 * i.e. probability of an interesting proposition ...

  *  given (conditional on) some data (i.e. whatever is fixed, either by observation or assumption)

 * Bayesian inference is a framework within which we construct a way to handle any problem involving uncertain propositions

 * Historical note: not clear that Thomas Bayes was a Bayesian

---

# What to do about uninteresting variables

 * What about variables which are neither interesting nor given??

 * Laws of probability dictate the Right Thing To Do:

  * namely, integrate joint probability of (interesting, not interesting) over (not interesting)

  * formally,
 
`\(P(\mathrm{interesting} | \mathrm{given}) = \int P(\mathrm{interesting}, \mathrm{uninteresting} | \mathrm{given}) d\mathrm{uninteresting}\)`

  * (where integral is a summation if uninteresting is discrete)
 
 * This is the motivation for the computational stuff we'll see later

---

# Some problems involving uncertain propositions

 * Missing data

 * Forecasts

 * Diagnosis

 * Model selection

 * Model parameters

 * "What if" scenarios

---

# Reasonable, non-Bayesian approaches

 * Missing data: plug in estimate of missing variable

 * Forecasts: output forecast = best guess

 * Diagnosis: output diagnosis = best guess

 * Model selection: choose "the best" model

 * Model parameters: choose best parameters

 * "What if" scenarios: plug in example inputs into model and get output
 
---

# Approaches as enriched by a Bayesian perspective

 * Missing data: consider all possible values of missing variable

 * Forecasts: consider all possible output values

 * Diagnosis: consider all possible diagnoses

 * Model selection: consider all possible models

 * Model parameters: consider all possible parameters

 * "What if" scenarios: consider all possible results

---

# A closer look at the model parameters problem

 * Goal is to construct posterior distribution of parameters given data

 * In some cases an exact distribution of parameters can be constructed exactly or approximately

 * In other cases, can't construct it explicitly but can construct an algorithm to sample from posterior

 * ... then to produce e.g. forecasts, sample from posterior and use parameters to generate output

 * ... and the result is a posterior distribution over the output

---

# Example problem: linear model with two free parameters

 * Model is `\(y = \alpha_0 + \alpha_1 x + \epsilon\)` with `\(\epsilon \sim N(0, \sigma_{\epsilon}^2\)`

 * Given data: `\((0.4, 1.2), (0.6, 0.8), (1.3, 1.5)\)`

 * Prior information about parameters: `\(p(\alpha_0) = \alpha_0 e^{-\alpha_0}, p(\alpha_1) = \alpha_1 e^{-\alpha_1}\)`

 * Likelihood function: `\(L(\alpha_0, \alpha_1) = p(\mathrm{data} | \alpha_0, \alpha_1) = \prod_{i=1}^3 p_{\epsilon}(y_i - (\alpha_0 + \alpha_1 x_i))\)`

 * Forecast: for a given `\(x\)`, what is `\(p(y | x, \mathrm{data})\)` ??

 * `\(p(y | x, \mathrm{data}) = \int \int p(y | x, \alpha_0, \alpha_1) p(\alpha_0, \alpha_1 | \mathrm{data}) d\alpha_0 d\alpha_1\)`

---

# Computing integrals -- (1) symbolic

 * Stuff like: `\(\int_0^x \alpha e^{-\alpha t} \beta e^{-\beta (x - t)} dt = \frac{\alpha \beta}{\beta - \alpha} \(e^{-\alpha x} - e^{-\beta x}\)\)`

 * Gives insight if assumptions are satisfied

 * Limited applicability

 * In R, use rsympy or maybe ryacas which are glue code for computer algebra systems

---

# Computing integrals -- (2) quadrature formulas

 * stats::integrate adaptive subdivision based on rules exact for nested refinements of polynomials

 * EXAMPLE:

 * Works well in 1 dimension, tolerable in a few more

 * Unworkable in many dimensions (a common application scenario)

---

# Computing integrals -- (3) simple Monte Carlo

 * runif, rnorm, rgamma, etc generate pseudorandom numbers

 * Approximate integral as average over samples

 * EXAMPLE: 

 * Easy to apply, very general, low computational efficiency

---

# Computing integrals -- (4) Markov chain Monte Carlo

 * More general than ordinary Monte Carlo

 * If one can't explicitly construct sampling distribution,
   construct Markov chain s.t. stationary distribution is desired distribution

 * Under some conditions, averages over time are equal to averages over space

 * MCMC slower than ordinary MC due to autocorrelation, but more general

---

# Markov chain Monte Carlo example

Given a function `L` proportional to target distribution:

```r
L1 &lt;- function (x, y) {exp(-((x - 2)^2 + 2*(y - 1)^2))}
L2 &lt;- function (x, y) {exp(-((x - 1)^2 + 0.5*(y - 2)^2))}
L &lt;- function (x, y) {L1(x, y) + L2(x, y)}
```

---

![Function proportional to target distribution](proportional_target.svg)

---

Proposal distribution which is just a Gaussian bump:

```r
Q &lt;- function (x,y) {rnorm (2, mean=c(x, y), sd=0.1)}
```

Generate a new proposed point:

```r
xy1 &lt;- Q (xy0[1], xy0[2])
```

Compute acceptance ratio:
```r
r &lt;- L (xy1[1], xy1[2]) / L (xy0[1], xy0[2])
```

Accept or reject proposal:
```r
if (r &gt; 1 || r &gt; runif(1)) {xy0 &lt;- xy1}
```

---

Generate a sequence of samples given a starting point `xy0`:

```r
mcmc.sequence &lt;- function (n, xy0) {
  x &lt;- vector (length=n)
  y &lt;- vector (length=n)
  x[1] &lt;- xy0[1]
  y[1] &lt;- xy0[2]
  for (i in 2:n) {
    xy1 &lt;- Q (xy0[1], xy0[2])
    r &lt;- L (xy1[1], xy1[2]) / L (xy0[1], xy0[2])
    if (r &gt; 1 || r &gt; runif(1)) {xy0 &lt;- xy1}
    x[i] &lt;- xy0[1]
    y[i] &lt;- xy0[2]
  }
  list (x=x, y=y)
}
```

---

!["Burn-in" sequence](target+initial-sequence.svg)

---

![Burn-in plus additional samples](target+initial+subsequent-sequence.svg)

---

# Stan, a package for Bayesian inference

 * R interface rstan

 * inference via "no U-turn sampling" MCMC

 * other capabilities -- it's a large package

 * previous example in Stan

---

# What use is a probability?

 * Rational approach is to make decision on basis of expected utility

  * Utility = quantify value or preference

  * Probability = quantify belief

  * Expected utility = utility integrated wrt probability distribution

 * Instead of using a probability by itself, consider expected gain/loss

 * e.g. P(change service) = 0.2; what should service provider do?

  * answer has to depend on what service provider stands to gain or lose

    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
