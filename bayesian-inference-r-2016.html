<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Bayesian Inference in R</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Introduction to Bayesian Inference in R
## Robert Dodier

This document and associated material is on Github:
[robert-dodier/bayesian-inference-r-2016](https://github.com/robert-dodier/bayesian-inference-r-2016)

---

# What is Bayesian inference?

 * Bayesian inference = laws of probability applied to any proposition

 * Proposition = statement about uncertain variables

 * Variables = observable quantities, model parameters, latent (hidden) variables, hypotheses, etc

 * Fundamental operation is to compute conditional probabilities

 * i.e. probability of an interesting proposition ...

  *  given (conditional on) some data (i.e. whatever is fixed, either by observation or assumption)

 * Bayesian inference is a framework within which we construct a way to handle any problem involving uncertain propositions

 * Historical note: not clear that Thomas Bayes was a Bayesian

---

# What to do about uninteresting variables

 * What about variables which are neither interesting nor given??

 * Laws of probability dictate the Right Thing To Do:

  * namely, integrate joint probability of (interesting, not interesting) over (not interesting)

  * formally,
 
`\(P(\mathrm{interesting} | \mathrm{given}) = \int P(\mathrm{interesting}, \mathrm{uninteresting} | \mathrm{given}) d\mathrm{uninteresting}\)`

  * (where integral is a summation if uninteresting is discrete)
 
 * This is the motivation for the computational stuff we'll see later

---

# Some problems involving uncertain propositions

 * Missing data

 * Forecasts

 * Diagnosis

 * Model selection

 * Model parameters

 * "What if" scenarios

---

# Reasonable, non-Bayesian approaches

 * Missing data: plug in estimate of missing variable

 * Forecasts: output forecast = best guess

 * Diagnosis: output diagnosis = best guess

 * Model selection: choose "the best" model

 * Model parameters: choose best parameters

 * "What if" scenarios: plug in example inputs into model and get output
 
---

# Approaches as enriched by a Bayesian perspective

 * Missing data: consider all possible values of missing variable

 * Forecasts: consider all possible output values

 * Diagnosis: consider all possible diagnoses

 * Model selection: consider all possible models

 * Model parameters: consider all possible parameters

 * "What if" scenarios: consider all possible results

---

# A closer look at the model parameters problem

 * Goal is to construct posterior distribution of parameters given data

 * In some cases an exact distribution of parameters can be constructed exactly or approximately

 * In other cases, can't construct it explicitly but can construct an algorithm to sample from posterior

 * ... then to produce e.g. forecasts, sample from posterior and use parameters to generate output

 * ... and the result is a posterior distribution over the output

---

# Example problem: linear model with two free parameters

 * Model is `\(y = \alpha_0 + \alpha_1 x + \epsilon\)` with `\(\epsilon \sim N(0, \sigma_{\epsilon}^2\)`

 * Given data: `\((0.4, 1.2), (0.6, 0.8), (1.3, 1.5)\)`

 * Prior information about parameters: `\(p(\alpha_0) = \alpha_0 e^{-\alpha_0}, p(\alpha_1) = \alpha_1 e^{-\alpha_1}\)`

 * Likelihood function: `\(L(\alpha_0, \alpha_1) = p(\mathrm{data} | \alpha_0, \alpha_1) = \prod_{i=1}^3 p_{\epsilon}(y_i - (\alpha_0 + \alpha_1 x_i))\)`

 * Forecast: for a given `\(x\)`, what is `\(p(y | x, \mathrm{data})\)` ??

 * `\(p(y | x, \mathrm{data}) = \int \int p(y | x, \alpha_0, \alpha_1) p(\alpha_0, \alpha_1 | \mathrm{data}) d\alpha_0 d\alpha_1\)`

---

# Computing integrals -- (1) symbolic

 * Stuff like: `\(\int_0^x \alpha e^{-\alpha t} \beta e^{-\beta (x - t)} dt = \frac{\alpha \beta}{\beta - \alpha} \(e^{-\alpha x} - e^{-\beta x}\)\)`

 * Gives insight if assumptions are satisfied

 * Limited applicability

 * In R, use rsympy or maybe ryacas which are glue code for computer algebra systems

---

# Computing integrals -- (2) quadrature formulas

 * stats::integrate adaptive subdivision based on rules exact for polynomials

---

# Linear regression example using quadrature

```r
py.given.x &lt;-
  function (y, x) {
    integrate (
      function (a.1) {
        sapply (a.1,
                function (a.1) {
                  integrate (function (a.0) {
                               py.given.x.a0.a1.sigma.e (y, x, a.0, a.1, 0.25) *
                                 P2 (a.0, a.1)}, 
                             -Inf, Inf)$value})},
      0, Inf)}
```

---

# Calculating a posterior distribution via quadrature

```r
yy &lt;- seq (from=0.4, to=2.1, by=0.01)
sapply (yy, function (y) {py.given.x (y, 0.75) $ value}) -&gt; pp
I &lt;- sum(pp)*0.01
pp &lt;- pp/I

plot (x=yy, y=pp, type="l")
```

![Posterior distribution of y given 0.75 via quadrature](y-given-x-via-quadrature.svg)

---

# About quadrature formulas

 * Works well in 1 dimension, tolerable in a few more

 * Unworkable in many dimensions (a common application scenario)

---

# Computing integrals -- (3) simple Monte Carlo

 * runif, rnorm, rgamma, etc generate pseudorandom numbers

 * Approximate integral as average over samples: `\(\int f(x) p(x) dx \approx \frac{1}{n} \sum_i f(x_i)\)`

---

# Linear regression example using simple Monte Carlo

```r
py.given.x.via.monte.carlo &lt;-
  function (y, x) {mean (mapply (function (a.0, a.1) {py.given.x.a0.a1.sigma.e (y, x, a.0, a.1, 0.25) * L(a.0, a.1, 0.25) * psigma.e(0.25)}, rnorm (1000, mean=0, sd=1), rgamma (1000, shape=2, scale=1)))}
```

----

# Calculating a posterior distribution via simple Monte Carlo

```r
pp &lt;- sapply (yy, function (y) {py.given.x.via.monte.carlo (y, 0.75)})
I &lt;- sum(pp)*0.01
pp &lt;- pp/I
```

![Posterior distribution of y given x=0.75 via simple Monte Carlo](y-given-x-via-monte-carlo.svg)

---

# About simple Monte Carlo

 * Easy to apply, may be inefficient
 
 * Limited applicability (difficulty of constructing sampling distribution)

  * ... this is the motivation for Markov chain Monte Carlo

---

# Computing integrals -- (4) Markov chain Monte Carlo

 * More general than ordinary Monte Carlo

 * If one can't explicitly construct sampling distribution,
   construct Markov chain s.t. stationary distribution is desired distribution

 * Under some conditions, averages over time are equal to averages over space

 * MCMC slower than ordinary MC due to autocorrelation, but more general

---

# Markov chain Monte Carlo example

Given a function `L` proportional to target distribution:

```r
L1 &lt;- function (x, y) {exp(-((x - 2)^2 + 2*(y - 1)^2))}
L2 &lt;- function (x, y) {exp(-((x - 1)^2 + 0.5*(y - 2)^2))}
L &lt;- function (x, y) {L1(x, y) + L2(x, y)}
```

---

![Function proportional to target distribution](proportional_target.svg)

---

Proposal distribution which is just a Gaussian bump:

```r
Q &lt;- function (x,y) {rnorm (2, mean=c(x, y), sd=0.1)}
```

Generate a new proposed point:

```r
xy1 &lt;- Q (xy0[1], xy0[2])
```

Compute acceptance ratio:
```r
r &lt;- L (xy1[1], xy1[2]) / L (xy0[1], xy0[2])
```

Accept or reject proposal:
```r
if (r &gt; 1 || r &gt; runif(1)) {xy0 &lt;- xy1}
```

---

Generate a sequence of samples given a starting point `xy0`:

```r
mcmc.sequence &lt;- function (n, xy0) {
  x &lt;- vector (length=n)
  y &lt;- vector (length=n)
  x[1] &lt;- xy0[1]
  y[1] &lt;- xy0[2]
  for (i in 2:n) {
    xy1 &lt;- Q (xy0[1], xy0[2])
    r &lt;- L (xy1[1], xy1[2]) / L (xy0[1], xy0[2])
    if (r &gt; 1 || r &gt; runif(1)) {xy0 &lt;- xy1}
    x[i] &lt;- xy0[1]
    y[i] &lt;- xy0[2]
  }
  list (x=x, y=y)
}
```

---

!["Burn-in" sequence](target+initial-sequence.svg)

---

![Burn-in plus additional samples](target+initial+subsequent-sequence.svg)

---

# Stan, a package for Bayesian inference

 * Stan has its own modeling language, primarily declarative

 * User states how variables are related, Stan figures out calculations

 * Inference via "no U-turn sampling" MCMC

 * Also implements variational inference and optimization functions

---

# Linear regression model in Stan

```
model {
  real mu;
  for (i in 1:3) {
    mu = alpha[1] + alpha[2] * x[i];
    y[i] ~ normal(mu, sigma_e);
  }
}

generated quantities {
  real y_pred;
  y_pred = normal_rng(alpha[1] + alpha[2] * new_x, sigma_e);
}
```

---

# Making inferences via RStan

```r
library (rstan)
stan.data &lt;- list (x=c(0.1, 0.5, 0.6), y=c(0.35, 0.9, 1.1), sigma_e=0.25, new_x=0.75)
my.stan.model &lt;- stan (file="model.stan", data=stan.data, iter=2000, chains=4)

traceplot (my.stan.model, pars = c("alpha"), inc_warmup = T)

y_pred_struct &lt;- extract (my.stan.model, 'y_pred')
y_pred &lt;- unlist (y_pred_struct, use.names=F)
hist (y_pred, freq=F, main="Density of y|x via Stan")
```

---

# Samples from the Markov chain for the parameters

![Samples from the Markov chain for the parameters](stan-model-posterior-samples.svg)

---

# Posterior distribution of y given x=0.75

![Posterior distribution of y given x=0.75](y-given-x-via-stan.svg)

---

# What use is a probability?

 * Rational approach is to make decision on basis of expected utility

  * Utility = quantify value or preference

  * Probability = quantify belief

  * Expected utility = utility integrated wrt probability distribution

 * Instead of using a probability by itself, consider expected gain/loss

 * e.g. P(change service|data) = 0.2; what should service provider do?

  * answer has to depend on what service provider stands to gain or lose

    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
